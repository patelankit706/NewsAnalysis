{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cf8967-143c-46aa-8488-c3041a7804d3",
   "metadata": {},
   "source": [
    "# Assignment Techdome, Indore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8d843-4133-4ac6-9035-89ffc35eeef2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Install libraries\n",
    "Run below cell to install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a8e96-5c00-4b6c-ab6e-f7562bfd1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas numpy openpyxl nltk bert-extractive-summarizer transformers langchain==0.1.4 openai==1.10.0 tiktoken python-dotenv scikit-learn langchain-openai\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2293cbb-4f97-4f53-8c19-8bb59c30c2d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import libraries\n",
    "Important libraries overview\n",
    "* `re`: modifying strings with regular expression \n",
    "* `nltk, spacy`: for text prerocessing and sentiment analysis\n",
    "* `pandas, numpy`: efficient data manipulation\n",
    "* `summarizer`: for extractive summarization\n",
    "* `dotenv`: load environment variables from .env file. Import openai api key OPENAI_API_KEY to environment variable\n",
    "* `langchain`: for prompt creation and interacting with openai api\n",
    "* `langchain.embeddings`: Create document embedding using openai embeddings model\n",
    "* `sklearn.cluster`: KMeans for clustering similar documents using document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3cacd0f5-98ea-48ea-9a4e-6216f5c2cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/patelankit706/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/patelankit706/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patelankit706/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from summarizer import TransformerSummarizer\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "#from langchain import LLMChain\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f609b-857e-4114-a6ff-61569d4e3a98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocess data\n",
    "Preprocess data using spacy, nltk. Removing stopwords, lowercasing, lemmatization, punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ccb32c1d-5c41-4575-9711-c61f73a6451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "df = pd.read_excel(\"Assignment.xlsx\")\n",
    "articles = df['Article'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "69cca913-2901-44fc-a6ac-97bbc1aad7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## punctuation and custom stop words\n",
    "stop_words = {'all', 'who', 'so', 'some', 'whom', 'have', 'any', 'did', 'be', 'me',\n",
    "              'mine', 'a', 'this', 'i', 'at', 'an', 'between', 'below', 'was', 'why',\n",
    "              'it', 'is', 'he', 'above', 'that', 'itself', 'or', 'does', 'on', 'here',\n",
    "              'as', 'the', 'has', 'down', 'for', 'until', 'of', 'own', 'other', 'do', 'both',\n",
    "              'same', 'if', 'while', 'she', 'to', 'were'}\n",
    "\n",
    "punctuation = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~\\n“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c84bd9b7-d5cc-4011-b373-23f4295fdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', phrase)\n",
    "    phrase = re.sub(r\"\\'t|’t\", \"not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re|’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s|’s|—\", \" \", phrase)\n",
    "    phrase = re.sub(r\"\\'d|’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll|’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve|’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m|’m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def preprocess_nltk(text, punctuation=\"\", stopwords=set(), decontracted=None, lemmatizer=False, lowercase=False):\n",
    "    if decontracted:\n",
    "        text = decontracted(text)\n",
    "    tokens = word_tokenize(text)    \n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    if lemmatizer:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        filtered_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    #sentence = re.sub(r\" \\.\",\".\",' '.join(lemmatized_tokens))\n",
    "    sentence = ' '.join(filtered_tokens)\n",
    "    if lowercase: sentence = sentence.lower()\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aafe81f-aac2-43f7-b95e-f6d6f1f43e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spacy(text, stop_words=set()):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Process whole documents\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, Lemmatization, Punctuation and Stop words removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and token.text not in stop_words]\n",
    "    \n",
    "    sentence= re.sub(r\"\\n+\",\"\",\" \".join(tokens))\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e24d22a-34c8-4a49-83ca-9f35dbed3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_articles_nltk_stopwords = [preprocess(i, punctuation, STOP_WORDS, None, True, True) for i in articles]\n",
    "preprocessed_articles_spacys = [preprocess_spacy(i, STOP_WORDS) for i in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d9dc8fc6-610d-4cab-899d-5fc22781a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sept 14 Reuters Bristol Myers Squibb BMY.N say Thursday plan double number treatment test clinical trial focus cell therapy 18 month contend increase generic competition sell drug  the drugmaker currently candidate trial advance research pipeline include cell therapy target immune system disorder different type cancer  the New York base company pressure decline demand drug blood cancer treatment Revlimid blood thin Eliquis face generic competition  Bristol partner Pfizer pfe.n blood thin Eliquis list 10 drug subject price negotiation U.S. Medicare health program  the company recently receive regulatory approval new cell therapy manufacturing facility Devens Massachusetts Bristol say continue expand manufacturing capacity  Bristol approve cell therapy U.S. Breyanzi Abecma target different blood cancer indication say plan continue development treatment disease lupus erythematosus multiple sclerosis  the drugmaker host R&D day Thursday executive expect provide detail company research strategy'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_articles_spacys[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c91ef-118f-4e8a-b8b0-1a04c1a12575",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### VADER sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85571fd7-31ba-4d3a-9bfb-32f0d9a0362b",
   "metadata": {},
   "source": [
    "Valence aware dictionary for sentiment reasoning (VADER) is popular rule-based sentiment analyzer. \n",
    "<br>\n",
    "It uses a list of lexical features (e.g. word) which are labeled as positive or negative according to their semantic orientation to calculate the text sentiment.   \n",
    "\n",
    "Vader sentiment returns the probability of a given input sentence to be \n",
    "Positive, negative, and neutral. \n",
    "\n",
    "For our task Vader will not provide good accuracy as its not trained on data specific to our domain requirement. For initial results lets go with this model only. For getting better result we can fine tune already available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efb9e493-dc77-4f19-8173-09a68952204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(score):\n",
    "    if score>0.05:\n",
    "        return \"Positive\"\n",
    "    elif score>-0.05:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1ae0e4a-fefc-4547-b270-11e0de0e6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_vader = [getSentiment(analyzer.polarity_scores(i.lower())['compound']) for i in preprocessed_articles_spacys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a487e4f6-9514-493d-9a91-5101bf17d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sentiment_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b51413c-3fdc-4572-a9e2-f06cbae766b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sentiment in dataframe\n",
    "df[\"vader_sentiment\"]=sentiment_vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159e598-3588-4929-8579-7b9928c4ec54",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "Lets get the summaries of our articles.<br>\n",
    "There can be following 3 approaches to get the summary:\n",
    "1. Extractive\n",
    "2. Abstractive\n",
    "3. Hybrid\n",
    "<br>\n",
    "We will go with Abstractive approach. An example of extractive summarization is given below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986a890-870d-4632-b240-807855c88202",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Extractive summarization\n",
    "Extractive summarization selects important sentences from the provided text in the summary without modification.<br>\n",
    "For this task we gre using gpt-2 summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b5f3353-564e-4c7a-b95f-57b433814969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom stop words\n",
    "preprocessed_articles = [preprocess_nltk(i, punctuation, stop_words, decontracted) for i in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cf1ab97-9ef7-4eba-9872-3e767d67db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patelankit706/anaconda3/envs/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "615a46a6-ec3a-471b-8ea0-62478fe43842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRUKINSA first and only BTK inhibitor approved follicular lymphoma in European Union Approval based results from ROSEWOOD trial in which BRUKINSA plus anti-CD20 monoclonal antibody obinutuzumab achieved higher overall response rate compared obinutuzumab alone BeiGene Ltd. BGNE HKEX 06160 SSE 688235 global biotechnology company today announced European Commission EC granted marketing authorization BRUKINSA® zanubrutinib in combination with obinutuzumab treatment adult patients with relapsed refractory R/R follicular lymphoma FL received least two prior lines systemic therapy . `` With approval we are excited announce BRUKINSA will become available treatment option patients with follicular lymphoma in European Union . `` results from ROSEWOOD trial demonstrated significant clinical benefit BRUKINSA plus obinutuzumab patients with relapsed refractory follicular lymphoma . global BRUKINSA development program includes more than 5,000 subjects enrolled date in 29 countries and regions . FL remains incurable people with condition can live long time . five-year survival rate about 90 percent and approximately half people diagnosed with FL can live with disease nearly 20 years.iii iv BRUKINSA small molecule inhibitor Bruton tyrosine kinase BTK discovered by BeiGene scientists currently being evaluated globally in broad clinical program monotherapy and in combination with therapies treat various B-cell malignancies . We are committed radically improving access medicines far more patients need them .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(GPT2_model(preprocessed_articles[8], min_length=60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597360d-dcb5-486e-b1df-390714a5e769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Abstractive summarization\n",
    "Abstractive summarization rephrases sentences for summary.\n",
    "<br>\n",
    "We will be using openai gpt-3 model api for summarization of the articles. Since some articles are of greater size than the context window of the llm. We can split documents into small chunks and find the summary for each chunk. We can then get final summary using the previous chunks combined summary. This method is called map reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9ba7a33-0751-406b-a7ca-f59e039881e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_llm_text = [re.sub(r\"[\\n]+\", \" \", i) for i in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c438366b-536d-4596-b86b-c7cf7e478a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=50, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be0b7663-8cdf-4b1f-a892-b158f0668d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(articles_llm_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "17bc4dcb-18db-4f2f-bafe-b0aa75355bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to study this,” Michael Johnson, the chief executive of the nutritional supplement maker Herbalife, told investors. “And when we see an opportunity to capitalize on it, we will.” In theory, that opportunity — both for making profits and for losing fortunes — could be vast not only for the companies behind these drugs but also for some in completely different industries. Known as GLP-1 drugs, the medications are already driving big profits. Novo Nordisk makes both Ozempic, which has been approved only for Type 2 diabetes, and its close relative Wegovy, which has been approved for weight loss. They mimic a glucagon-like peptide that regulates appetite in the brain, leaving people feeling sated for hours. Together, they helped send Novo’s earnings rocketing up 32 percent in the first half of this year, and Novo’s market value is now larger than the entire Danish economy. Eli Lilly’s sales surged 28 percent in the second quarter, thanks to another diabetes drug, Mounjaro, which the Food'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7ed3df7-1cd1-4e56-9381-00f82f6d4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d513bfe-a4bf-4329-add2-16cbe3b42176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patelankit706/anaconda3/envs/py310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "321be3e3-cc9f-4c97-bdbd-4424c00c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The rise of diabetes and weight loss drugs like Ozempic has had an impact on retailers and food\n",
      "manufacturers. Companies like Herbalife and Novo Nordisk have seen significant increases in earnings\n",
      "and market value due to the success of these drugs. The FDA may approve a new weight loss drug,\n",
      "Mounjaro, this year, leading to increased sales for retailers. However, these drugs come with side\n",
      "effects and there is still room for other approaches to fighting obesity. The departure of Emily\n",
      "Weiss as CEO of Glossier has sparked discussion about the \"girlboss\" archetype in media and the\n",
      "challenges faced by female-led companies. Glossier has undergone changes and entered the retail\n",
      "market in preparation for a potential exit.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm,chain_type=\"map_reduce\")\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc0b5422-344b-45c6-bdfd-5e938e6bc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_articles_docs(articles, chain, text_splitter):\n",
    "    docs_list = [[Document(page_content=t) for t in text_splitter.split_text(article)] for article in articles_llm_text]\n",
    "    summaries = [chain.run(i) for i in docs_list]\n",
    "    return docs_list, summaries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28fbffdf-0c2d-464b-bbbc-dac99d45d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list, summaries = summarize_all_articles_docs(articles_llm_text, chain, text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7e60983-ead8-4d8f-a27e-128db0608d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Get Smart About News is a free newsletter and e-learning platform that helps educators teach students about identifying credible information and understanding the First Amendment. A recent investigation found that registered dietitians on social media were not transparent about their partnerships with food and beverage companies. China has been using disinformation campaigns to spread false information about the Hawaii wildfires, highlighting the importance of recognizing and addressing false claims. News literacy skills, like reverse image searches, can prevent people from falling for conspiracy theories. The claim about airlines using odd seating is false and an example of slippery slope thinking used to spread misinformation.\"'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ed25424-db04-4c42-8fbb-2d7961b95ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save abstractive summaries in dataframe\n",
    "df[\"abstractive_summary\"]=summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b08ea2-4ab4-4717-8603-f0b5d4374113",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sentiment of the articles using Few Shot Learning\n",
    "We will now use llm for sentiment analysis of the articles.<br>\n",
    "In this case we are using few shot learning for providing context to llm model with some examples.\n",
    "\n",
    "We are using the abstractive summaries(previously generated), for getting sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78e13708-0a97-412d-9911-0dd32dc2b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Scientists develop a new solar panel that's more efficient than ever before. This could help us fight climate change!\",\n",
    "        \"answer\": \"Positive\"\n",
    "    }, {\n",
    "        \"query\": \"The economy is struggling, and many people are losing their jobs. This is a worrying trend.\",\n",
    "        \"answer\": \"Negative\"\n",
    "    }, {\n",
    "        \"query\": \"A new report details the latest scientific findings on climate change. The report is informative and well-written.\",\n",
    "        \"answer\": \"Neutral\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for providing sentiment whether the user's input paragraph is Neutral, Positive or Negative. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8202806-e115-48be-b4ab-c6e6a81ceccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "chain.run(summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42cd75a8-85eb-40d4-914d-b89e8b43ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [chain.run(i) for i in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29912961-3a3f-4ef2-bfe9-17e91dc4b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bb9674f-2766-46f0-b3ae-08fc57278d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentiment output of llm in dataframe\n",
    "df[\"sentiment_gpt\"] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c64bba-1dde-4e69-96c8-229de7a169c7",
   "metadata": {},
   "source": [
    "### Common theme between articles\n",
    "There are various methods through with we can find common topics or themes between texts like LDA for unsupervised topic decomposition.\n",
    "\n",
    "Here we are using embeddings, custering and LLM together to get the most representative common theme of the different clusters of article.\n",
    "\n",
    "Methodology:\n",
    "1. We will divide each documents or article into small chunk. Also keep track of the article to which particular chunk belongs.\n",
    "2. Find the embeddings of all the chunks of all articles\n",
    "3. Utilize kmeans clustering for grouping of chunks in clusters.\n",
    "4. Find the articles index present in each cluster utilizing the article index tracker we created in step 1.\n",
    "5. For each cluster utilize llm map reduce to find the common theme in each cluster of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccc810-1c12-479a-8528-86cc7280dcaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. We will divide each documents or article into small chunk. Also keep track of the article to which particular chunk belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d07d71a-21b4-4903-8ece-339a7cb0a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide each documents or article into small chunk\n",
    "text_splitter_2 = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50, separators=[\" \", \",\", \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90be1db8-13b9-4c7b-9127-7e187759ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep track of the article to which particular chunk belongs\n",
    "index_text = []\n",
    "splitted_texts = []\n",
    "for i, article in enumerate(preprocessed_articles_spacys):\n",
    "    for t in text_splitter_2.split_text(article):\n",
    "        splitted_texts.append(t)\n",
    "        index_text.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a4296-3283-4229-9e28-3d35c7626390",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Find the embeddings of all the chunks of all articles\n",
    "\n",
    "We are using Open ai embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5eee9328-d4d9-4794-bece-c6ba727f65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36323d0d-cff7-493b-81ba-8cea2125877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embedding = embeddings.embed_documents(splitted_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f74dd5-0f4a-4256-9d2a-593e278f238a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Utilize kmeans clustering for grouping of chunks in clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d073764a-4c55-49ce-9284-2ddd28719dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_chunks = KMeans(n_clusters=6, random_state=45, n_init=\"auto\").fit(chunks_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "760f40dd-5da1-4a70-ba39-967bdada646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2,\n",
       "       2, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 4, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_chunks.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93cefd0-1605-4d80-8c70-93d06fab967d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4. Find the articles index present in each cluster utilizing the article index tracker we created in step 1 \"index_text\"\n",
    "<br> One Article can be present in multiple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "79b640fa-2a0b-4b96-8f1f-848627cd5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_article = {}\n",
    "cluster_to_chunks = {}\n",
    "for index, cluster in enumerate(kmeans_chunks.labels_):\n",
    "    if cluster in cluster_to_article:\n",
    "        cluster_to_article[cluster] = cluster_to_article[cluster] | {index_text[index],}\n",
    "        cluster_to_chunks[cluster] = cluster_to_chunks[cluster]+[index]\n",
    "    else:\n",
    "        cluster_to_article[cluster] = {index_text[index],}\n",
    "        cluster_to_chunks[cluster] = [index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8844ea75-8a0b-4c85-b224-efcaa76142b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {0, 1, 2, 7, 8, 10, 20, 21, 22, 23},\n",
       " 4: {0, 15, 20},\n",
       " 1: {2, 3, 4, 5, 6, 8},\n",
       " 5: {9, 10, 11, 12, 13, 14, 15, 16, 17},\n",
       " 3: {18, 19, 20},\n",
       " 0: {24}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mapping of each cluster to the the respective article index\n",
    "cluster_to_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ebe4eb9d-a48a-4a7a-a11a-19e76da6e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_clusters = np.zeros((6,len(articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89643055-c0aa-45ec-85ef-211f6f03b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_no, articles_indexes in cluster_to_article.items():\n",
    "    for i in articles_indexes:\n",
    "        one_hot_clusters[cluster_no, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "780c13dc-cee4-4678-912d-88e452ddee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "993ead46-1b29-4fcf-bc89-685489017c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in dataframe whether a particular article is present in a cluster or not\n",
    "for i in range(6):\n",
    "    df[f\"cluster_{i}\"] = one_hot_clusters[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55099ad5-0196-4898-b4de-91c2426873b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5. For each cluster utilize llm map reduce to find the common theme in each cluster of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "761944a1-593c-4668-a848-0a0e0b2c2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7e91cef1-1bbd-4300-949d-43cf47ca2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and find the common theme across these summaries. Do not provide description. There can be multiple common themes among those.\n",
    "Focus on the main theme for example Advancements in AI, Digital transformation in healthcare, etc. Don't return description. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f039c40e-20f1-4b1a-8d17-cd02b69890f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0daa6254-41b9-4c25-9200-282d88294133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c13aec17-af10-4a8a-b677-2fe9e5513ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_3 = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=50, separators=[\" \", \",\", \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f151c4e6-5d36-49b9-830a-805426f939e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_theme(cluster_index):\n",
    "    cluster_summaries = [Document(page_content=summaries[i]) for i in cluster_to_article[cluster_index]]\n",
    "    split_docs = text_splitter_3.split_documents(cluster_summaries)\n",
    "    print(map_reduce_chain.run(split_docs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8201eb5f-b48e-4fd4-b4ec-320236be8cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster_0 Theme:\n",
      "\n",
      "Bank promotions and account requirements\n",
      "\n",
      "\n",
      "Cluster_1 Theme:\n",
      "\n",
      "Advancements in cancer treatment and therapies.\n",
      "\n",
      "\n",
      "Cluster_2 Theme:\n",
      "\n",
      "Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation.\n",
      "\n",
      "\n",
      "Cluster_3 Theme:\n",
      "\n",
      "Partnerships and collaborations in the fitness and wellness industry.\n",
      "\n",
      "\n",
      "Cluster_4 Theme:\n",
      "\n",
      "Financial performance and strategies of major companies in various industries.\n",
      "\n",
      "\n",
      "Cluster_5 Theme:\n",
      "\n",
      "Advancements in technology and innovation in the food industry.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f\"Cluster_{i} Theme:\\n\")\n",
    "    get_cluster_theme(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3b46c184-a25a-41d3-8eae-36bdf95044e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_by_cluster_index = [\"Bank promotions and account requirements\",\n",
    "                         \"Advancements in cancer treatment and therapies\",\n",
    "                         \"Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation\",\n",
    "                         \"Partnerships and collaborations in the fitness and wellness industry\",\n",
    "                         \"Financial performance and strategies of major companies in various industries\",\n",
    "                         \"Advancements in technology and innovation in the food industry\"\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28903d-ac50-4e62-9325-c6ccbf1f3cc8",
   "metadata": {},
   "source": [
    "Map each article to their common theme. There can be multiple themes for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0607e8d0-f888-4ebb-9d81-24f143ca99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_themes = [\"\"]*len(articles)\n",
    "for i in range(len(articles)):\n",
    "    for cluster, articles_set in cluster_to_article.items():\n",
    "        if i in articles_set:\n",
    "            if article_themes[i]==\"\":\n",
    "                article_themes[i] = theme_by_cluster_index[cluster]\n",
    "            else:\n",
    "                article_themes[i] = article_themes[i]+\"|\"+theme_by_cluster_index[cluster]\n",
    "            #article_themes[i] = article_themes[i]+\"|\"+theme_by_cluster_index[cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "957ef5e0-288c-4676-9141-e525462ea125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation|Financial performance and strategies of major companies in various industries',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation|Advancements in cancer treatment and therapies',\n",
       " 'Advancements in cancer treatment and therapies',\n",
       " 'Advancements in cancer treatment and therapies',\n",
       " 'Advancements in cancer treatment and therapies',\n",
       " 'Advancements in cancer treatment and therapies',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation|Advancements in cancer treatment and therapies',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation|Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Financial performance and strategies of major companies in various industries|Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Advancements in technology and innovation in the food industry',\n",
       " 'Partnerships and collaborations in the fitness and wellness industry',\n",
       " 'Partnerships and collaborations in the fitness and wellness industry',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation|Financial performance and strategies of major companies in various industries|Partnerships and collaborations in the fitness and wellness industry',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation',\n",
       " 'Healthcare advancements, Pharmaceutical industry developments, Legal issues in advertising, Brand management and marketing, Disinformation and misinformation',\n",
       " 'Bank promotions and account requirements']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1296d5f6-0cdc-4e12-b609-2225b78d2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save article themes in dataframe\n",
    "df[\"common_theme\"] = article_themes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5d098-8555-4dfd-8fc4-2d204aa2fd2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ce2700cd-3f69-4c4f-8613-ff75b74a15e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>abstractive_summary</th>\n",
       "      <th>sentiment_gpt</th>\n",
       "      <th>cluster_0</th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>cluster_3</th>\n",
       "      <th>cluster_4</th>\n",
       "      <th>cluster_5</th>\n",
       "      <th>common_theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nike (NYSE:NKE) is the leader when it comes to...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Nike is a leading company in the sportswear i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Healthcare advancements, Pharmaceutical indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Oct 23 (Reuters) - Unilever (ULVR.L) and a cha...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unilever and a charity supporting teenage can...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Healthcare advancements, Pharmaceutical indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LONDON, Sept 19 (Reuters) - Nestle said on Tue...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Nestle has selected WPP Openmind as its exclu...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Healthcare advancements, Pharmaceutical indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Get Smart About News, modeled on the Sift, is ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>\"Get Smart About News is a free newsletter an...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Healthcare advancements, Pharmaceutical indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Editorial Note: Blueprint may earn a commissio...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Wells Fargo is currently offering promotions ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bank promotions and account requirements</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Article vader_sentiment  \\\n",
       "20  Nike (NYSE:NKE) is the leader when it comes to...        Positive   \n",
       "21  Oct 23 (Reuters) - Unilever (ULVR.L) and a cha...        Negative   \n",
       "22  LONDON, Sept 19 (Reuters) - Nestle said on Tue...        Positive   \n",
       "23  Get Smart About News, modeled on the Sift, is ...        Positive   \n",
       "24  Editorial Note: Blueprint may earn a commissio...        Positive   \n",
       "\n",
       "                                  abstractive_summary sentiment_gpt  \\\n",
       "20   Nike is a leading company in the sportswear i...      Positive   \n",
       "21   Unilever and a charity supporting teenage can...       Neutral   \n",
       "22   Nestle has selected WPP Openmind as its exclu...      Positive   \n",
       "23   \"Get Smart About News is a free newsletter an...       Neutral   \n",
       "24   Wells Fargo is currently offering promotions ...       Neutral   \n",
       "\n",
       "    cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  \\\n",
       "20          0          0          1          1          1          0   \n",
       "21          0          0          1          0          0          0   \n",
       "22          0          0          1          0          0          0   \n",
       "23          0          0          1          0          0          0   \n",
       "24          1          0          0          0          0          0   \n",
       "\n",
       "                                         common_theme  \n",
       "20  Healthcare advancements, Pharmaceutical indust...  \n",
       "21  Healthcare advancements, Pharmaceutical indust...  \n",
       "22  Healthcare advancements, Pharmaceutical indust...  \n",
       "23  Healthcare advancements, Pharmaceutical indust...  \n",
       "24           Bank promotions and account requirements  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "67e6f145-baf0-4471-ae2d-d263064884ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data \n",
    "df.to_excel(\"Final.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
